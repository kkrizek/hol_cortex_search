{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "Title",
    "collapsed": false
   },
   "source": "# ❄️ Snowflake Chat with your Documents Notebook ❄️\n\nIncludes:\n- Cortex Parse Document\n- Cortex Search Service\n- Cortex Fine-Tuning"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Flow",
    "collapsed": false,
    "codeCollapsed": true
   },
   "source": "# Import necessary functions\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Define image in a stage and read the file\nimage=session.file.get_stream(\"@DATASCIENCECOLLEGE.PUBLIC.RAG/RAG_flow.png\" , decompress=False).read() \n\n# Display the image\nst.image(image, width=1000)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Libraries",
    "collapsed": false
   },
   "source": "import snowflake.snowpark as snowpark\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "64842e12-1e4c-423b-a568-376102123485",
   "metadata": {
    "language": "sql",
    "name": "ViewStage",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- List files in the stage to identify PDFs\nLS @DATASCIENCECOLLEGE.PUBLIC.RAG;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f293bf7-06b5-4d05-9666-ad3096d25a31",
   "metadata": {
    "language": "sql",
    "name": "CreateParsedTextTable",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE PARSED_TEXT (relative_path VARCHAR(500), raw_text VARIANT);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "026e7c0b-6c8f-472f-9cfe-94e1351b9925",
   "metadata": {
    "language": "python",
    "name": "UseParseDocument",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, to_variant\n\n# Query to fetch distinct PDF files from the stage\nfiles_df = session.sql(\"\"\"\n    SELECT DISTINCT METADATA$FILENAME AS file_name\n    FROM @DATASCIENCECOLLEGE.PUBLIC.RAG\n    WHERE METADATA$FILENAME ILIKE '%.pdf'\n\"\"\").collect()\n\n# Loop through the distinct filenames and parse if not already in the target table\nfor row in files_df:\n    file_name = row['FILE_NAME']\n    \n    # Check if the file has already been parsed\n    check_df = session.table(\"PARSED_TEXT\").filter(col(\"relative_path\") == file_name).select(\"relative_path\").collect()\n\n    # If not already parsed, proceed to parse and insert the text\n    if not check_df:\n        # Extract raw text using the PARSE_DOCUMENT function\n        parse_result = session.sql(f\"\"\"\n            SELECT SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n                '@DATASCIENCECOLLEGE.PUBLIC.RAG',\n                '{file_name}',\n                OBJECT_CONSTRUCT('mode', 'OCR')\n            ) AS raw_text\n        \"\"\").collect()\n        \n        # Ensure parse_result contains data before proceeding\n        if parse_result:\n            # Get the parsed raw text and cast it to VARIANT\n            raw_text = parse_result[0]['RAW_TEXT']\n            \n            # Create DataFrame with explicit VARIANT type for raw_text\n            df_to_insert = session.create_dataframe(\n                [(file_name, raw_text)],\n                schema=[\"relative_path\", \"raw_text\"]\n            ).select(\n                col(\"relative_path\"),\n                to_variant(col(\"raw_text\")).alias(\"raw_text\")  # Explicitly cast to VARIANT\n            )\n            \n            # Insert the DataFrame into the PARSED_TEXT table\n            df_to_insert.write.mode(\"append\").save_as_table(\"PARSED_TEXT\")\n\nprint(\"PDF files parsed successfully.\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab172716-b5c1-4479-89fb-f362283c57b8",
   "metadata": {
    "name": "AlternateApproach",
    "collapsed": false
   },
   "source": "Here's an alternative approach using SQL to create and then call a procedure using the PARSE_DOCUMENT function."
  },
  {
   "cell_type": "markdown",
   "id": "4013c4f1-5a7e-4e33-b679-e038f44a2f8c",
   "metadata": {
    "name": "CreateParseDocumentProcedure",
    "collapsed": false
   },
   "source": " -- Convert this cell to SQL. Remove the quotes at the top & bottom to run the code.\n ```\n -- Create a procedure that will use PARSE_DOCUMENT to parse distinct PDFs\n CREATE OR REPLACE PROCEDURE parse_pdf()\n   RETURNS STRING NOT NULL\n   LANGUAGE JAVASCRIPT\n   EXECUTE AS CALLER\n AS\n $$\n   var file_name;\n   var rs = snowflake.execute({\n     sqlText: `\n       SELECT DISTINCT METADATA$FILENAME AS file_name\n       FROM @DATASCIENCECOLLEGE.PUBLIC.RAG\n       WHERE METADATA$FILENAME ILIKE '%.pdf'\n         AND POSITION('.pdf' IN LOWER(METADATA$FILENAME)) > 0\n     `\n   });\n\n   while (rs.next()) {\n     file_name = rs.getColumnValue(1);\n\n     // Check if the file has already been parsed\n     var check_rs = snowflake.execute({\n       sqlText: `\n         SELECT 1 \n         FROM PARSED_TEXT \n         WHERE relative_path = ?\n         LIMIT 1\n       `,\n       binds: [file_name]\n     });\n\n     // If the file has not been processed, parse it and insert the result\n     if (!check_rs.next()) {\n       snowflake.execute({\n         sqlText: `\n           INSERT INTO PARSED_TEXT (relative_path, raw_text)\n           SELECT ?, SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n             '@DATASCIENCECOLLEGE.PUBLIC.RAG',\n             ?,\n             OBJECT_CONSTRUCT('mode', 'OCR')\n           ) AS raw_text\n         `,\n         binds: [file_name, file_name]\n       });\n     }\n   }\n   return 'PDF files parsed successfully.';\n $$;\n ```"
  },
  {
   "cell_type": "markdown",
   "id": "c5054ad6-0966-40a0-881f-d8377449c8d8",
   "metadata": {
    "name": "CallParseProcedure",
    "collapsed": false
   },
   "source": "-- Convert this to SQL. Remove the quotes at the top & bottom to run the code.\n```\n-- Use procedure to create one record for each PDF in the stage\n-- CALL parse_pdf();\n```"
  },
  {
   "cell_type": "code",
   "id": "71fc5ddc-33ae-40d9-a36e-49de52d35de2",
   "metadata": {
    "language": "sql",
    "name": "ViewParsedData",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT RELATIVE_PATH, RAW_TEXT FROM PARSED_TEXT LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b1132c1-203a-406e-a1c1-d9a24dd55199",
   "metadata": {
    "name": "TestingNotes",
    "collapsed": false
   },
   "source": "Now let's chunk our parsed data using langchain (this will be updated to leverage Cortex function when available)."
  },
  {
   "cell_type": "code",
   "id": "4469e412-b254-4e86-9c21-075bf9b25aef",
   "metadata": {
    "language": "python",
    "name": "CreateTextChunker",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.types import StructType, StructField, StringType\nimport pandas as pd\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Define the text chunker class\nclass text_chunker:\n\n    def process(self, text):        \n        text_raw = []\n        text_raw.append(text) \n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            separators=[\"\\n\"],  # Define an appropriate separator. New line is good typically!\n            chunk_size=1000,     # Adjust this as you see fit\n            chunk_overlap=200,    # This lets text have some form of overlap. Useful for keeping chunks contextual\n            length_function=len,\n            add_start_index=True  # Optional but useful if you'd like to feed the chunk before/after\n        )\n    \n        chunks = text_splitter.create_documents(text_raw)\n        \n        # Adjust DataFrame creation to match schema (chunk, meta)\n        chunk_texts = [chunk.page_content.encode('utf-8', 'ignore').decode('utf-8') for chunk in chunks]\n        metas = [str(chunk.metadata) for chunk in chunks]\n        \n        df = pd.DataFrame({\n            'chunk': chunk_texts,\n            'meta': metas\n        })\n        \n        yield from df.itertuples(index=False, name=None)\n\n# Register the UDTF\nschema = StructType([\n     StructField(\"chunk\", StringType()),\n     StructField(\"meta\", StringType()),\n ])\n\nsession.udtf.register( \n    handler=text_chunker,\n    output_schema=schema, \n    input_types=[StringType()], \n    is_permanent=True, \n    name='CHUNK_TEXT', \n    replace=True, \n    packages=['pandas', 'langchain'], \n    stage_location='@DATASCIENCECOLLEGE.PUBLIC.RAG'\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d018fc83-2563-432e-8c69-598069fe57b3",
   "metadata": {
    "language": "sql",
    "name": "ChunkParsedText",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- Create the chunked version of your parsed text\nCREATE OR REPLACE TABLE DATASCIENCECOLLEGE.PUBLIC.CHUNK_TEXT AS\n    SELECT\n        raw.relative_path,\n        build_scoped_file_url('@DATASCIENCECOLLEGE.PUBLIC.RAG', raw.relative_path) AS file_url,\n        CONCAT(raw.relative_path, ': ', func.chunk) AS chunk,\n        'English' AS language,\n        func.meta AS meta_info\n    FROM\n        DATASCIENCECOLLEGE.PUBLIC.PARSED_TEXT AS raw,\n        TABLE(DATASCIENCECOLLEGE.PUBLIC.CHUNK_TEXT(TO_VARCHAR(raw.raw_text))) AS func;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00570c35-4b44-48c7-8586-eaaa7bebda52",
   "metadata": {
    "language": "sql",
    "name": "ViewChunkedText",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM CHUNK_TEXT LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44a330f9-c774-47f2-b8ca-031ae441c602",
   "metadata": {
    "language": "sql",
    "name": "CreateCortexSearchService",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- Create a search service over your new chunked pdf table\nCREATE OR REPLACE CORTEX SEARCH SERVICE DATASCIENCECOLLEGE.PUBLIC.CHUNK_TEXT_SEARCH_SERVICE\n    ON CHUNK\n    ATTRIBUTES LANGUAGE\n    WAREHOUSE = DATASCIENCECOLLEGE\n    TARGET_LAG = '365 days'\n    AS (\n    SELECT\n        CHUNK,\n        RELATIVE_PATH,\n        LANGUAGE\n    FROM DATASCIENCECOLLEGE.PUBLIC.CHUNK_TEXT\n    );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26870615-a19d-4da2-a7a4-5ee6d59cc448",
   "metadata": {
    "name": "QACustomizedModel",
    "collapsed": false
   },
   "source": "# Next Step:\n\nPlease create the associated \"Chat with your Documents\" SiS app. This will allow users to interactively leverage the Cortex Search RAG.\n\nOnce users have saved sufficient questions and customized/corrected answers via the SiS app, return to this Snowflake Notebook to create a customized fine-tuned Cortex model.\n"
  },
  {
   "cell_type": "code",
   "id": "c693d9c4-ca51-4c73-b143-6f9cc77dc19b",
   "metadata": {
    "language": "sql",
    "name": "SplitQAForModeling",
    "collapsed": false
   },
   "outputs": [],
   "source": "ALTER TABLE QA_TABLE\nADD COLUMN source VARCHAR;\nUPDATE QA_TABLE\nSET\n  source = CASE\n    WHEN RANDOM () <= 0.7 THEN 'train'\n    ELSE 'validation'\n  END;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "803467a8-c97d-4cec-9c77-2b96f0907fa0",
   "metadata": {
    "language": "sql",
    "name": "CreateFineTunedModel",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT\n  SNOWFLAKE.CORTEX.FINETUNE (\n    'CREATE',\n    'customized_QA_model',\n    'mistral-7b',\n    'SELECT RAGQUESTION as prompt, RAGANSWER as completion FROM QA_TABLE WHERE source = \\'train\\'',\n    'SELECT RAGQUESTION as prompt, RAGANSWER as completion FROM QA_TABLE WHERE source = \\'validation\\''\n  );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a28ae1b-60cd-42df-bb15-237b8cafef5f",
   "metadata": {
    "name": "UseCustomizedModel",
    "collapsed": false
   },
   "source": "# Next Step:\n\nReturn to the \"Chat with your Documents\" SiS App. \n\nUpdate lines 12 to 17, with this code to include your customized_QA_model:\n\n```\nENABLED_CUSTOM_QA_MODELS = True\n\nMODELS = [\n    \"mistral-large\",\n    \"snowflake-arctic\",\n    \"llama3-70b\",\n    \"llama3-8b\",\n    \"customized_QA_model\"\n]\n\nif ENABLED_CUSTOM_QA_MODELS:\n    MODELS.append( \"customized_QA_model\")\n```\n\nNow update lines 101 to 137, with an updated def init_config_options function. This will provide a checkbox for the user to select to use their custom model.:\n\n```\ndef init_config_options():\n    st.sidebar.selectbox(\n        \"Select Cortex Search Service:\",\n        [s[\"name\"] for s in st.session_state.service_metadata],\n        key=\"selected_cortex_search_service\",\n    )\n\n    clear_button_clicked = st.sidebar.button(\"Clear conversation\")\n    if clear_button_clicked:\n        st.session_state.clear_conversation = True\n        init_messages()\n        st.session_state.generated_response = \"\"\n        st.session_state.results = []\n        st.session_state.pdf_filename = None\n        st.session_state.selected_question_key += 1  # Increment key to reset selected question\n        st.session_state.question_key += 1  # Increment key to reset user question\n        st.session_state.user_question = \"\"  # Reset the user-defined question to blank\n\n    use_chat_history = st.sidebar.checkbox(\n        \"Use chat history\", value=st.session_state.use_chat_history\n    )\n    st.session_state.use_chat_history = use_chat_history\n\n    with st.sidebar.expander(\"Advanced options\"):\n        st.selectbox(\"Select model:\", MODELS, key=\"model_name\")\n        st.number_input(\n            \"Select number of context chunks\",\n            key=\"num_retrieved_chunks\",\n            min_value=1,\n            max_value=10,\n        )\n        st.number_input(\n            \"Select number of messages to use in chat history\",\n            key=\"num_chat_messages\",\n            min_value=1,\n            max_value=10,\n        )\n        use_custom_model = False\n        if ENABLED_CUSTOM_QA_MODELS:\n            # Add a checkbox to use customized Q&A model\n            use_custom_model = st.checkbox(\"Use customized Q&A model\", key=\"use_customized_qa_model\")\n\n    # Determine which model is currently active\n    st.session_state.active_model = \"customized_QA_model\" if use_custom_model else st.session_state.model_name\n```\n\nNow at line 197, update the complete function to use the customized_QA_model if the use selects the \"Use customized model\" checkbox.\n```\ndef complete(model, prompt):\n    # Use customized Q&A model if selected\n    if st.session_state.get(\"use_customized_qa_model\", False):\n        model = \"customized_QA_model\"\n    return Complete(model, prompt).replace(\"$\", \"\\$\")\n```\n\nFinally, select the \"Run\" button for your new code to be used and test your update application. Remember to look in the sidebar \"Advanced options\" to check the \"Use customized Q&A model\" when testing new questions. "
  }
 ]
}