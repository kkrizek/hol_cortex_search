{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65200ba8-622c-4850-9a6d-9d4501f768d3",
   "metadata": {
    "name": "Introduction",
    "collapsed": false,
    "resultHeight": 1040
   },
   "source": "# ❄️ Snowflake Chat with your Documents Notebook ❄️\n\nIncludes:\n- Cortex Recursive Split Text Character Parse Document\n- Foundation LLMs\n- Cortex Search Service\n- Building the Pipeline\n- Creating the App"
  },
  {
   "cell_type": "code",
   "id": "9ccf047d-11f2-40e9-b6f3-f2b8589c6056",
   "metadata": {
    "language": "python",
    "name": "Get_Active_Session",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Import necessary functions\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17099e62-1d65-4333-b427-91b49ead3994",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "/**************************\n Set the context - update the database\n**************************/\nuse database mdt2_cortex_search_docs#;\nuse schema data;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb896aba-accb-4989-a0dd-5556c87a7703",
   "metadata": {
    "language": "sql",
    "name": "copy_img",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "/*******************************************\nCreate a stage to hold the RAG architecture image\n\nChange the database in the INTO and FROM before running\n*******************************************/\nCREATE or replace STAGE images                                              \n  DIRECTORY = (enable = true)\n  ENCRYPTION = (type = 'snowflake_sse');\nCOPY FILES\n  INTO @images                                                              \n  FROM @mdt2_cortex_search_docs#.public.mdt2_snowflake_extensions/branches/main/images/RAG_flow.png;      --CHANGE THIS",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14899439-6d1b-480d-932c-0e820f91e4d9",
   "metadata": {
    "language": "python",
    "name": "RAG_Architecture",
    "collapsed": false,
    "resultHeight": 513,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define image in a stage and read the file\nimage=session.file.get_stream(\"@IMAGES/RAG_flow.png\" , decompress=False).read() \n\n# Display the image\nst.image(image, width=1000)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78661fb4-2edf-4e70-b734-3fa78a6518e1",
   "metadata": {
    "language": "sql",
    "name": "Create_DOCS_Stage",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "/******************************\nCreate the stage to host your documents.  We can see that the stage is empty.\n******************************/\nCREATE or replace STAGE docs              --CHANGE THIS\n  DIRECTORY = (enable = true)\n  ENCRYPTION = (type = 'snowflake_sse');\nls @docs;                                 --CHANGE THIS",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e14d946b-2bc5-4f1d-98f5-0e6a963a0c53",
   "metadata": {
    "language": "sql",
    "name": "Create_Stream",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "/******************************\nWe are preparing to operationalize the pipeline.  In order to do so, we need to create a STREAM that 'listens' for new documents to land in the stage.  We can see that the stream is empty.\n******************************/\ncreate or replace stream cortex_search_stream on stage docs;\nselect * from cortex_search_stream;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21dd7a79-ddb5-4f46-8e49-7e0000695c02",
   "metadata": {
    "language": "sql",
    "name": "Git_Repo_Documents",
    "collapsed": false,
    "resultHeight": 357
   },
   "outputs": [],
   "source": "/******************************\nAll of the files that we need are in the Git repository\n\nChange the name of the database\n******************************/\nls @mdt2_cortex_search_docs#.public.mdt2_snowflake_extensions/branches/main/documents;                --CHANGE THIS",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b088e84c-8dd6-438e-a39f-7306d68ab3d3",
   "metadata": {
    "language": "sql",
    "name": "Copy_Files_To_Stage",
    "collapsed": false,
    "resultHeight": 217
   },
   "outputs": [],
   "source": "/******************************\nCopy 4 of the files from the GIT repository to the Stage\n\nChange the name of the database\n******************************/\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs#.public.mdt2_snowflake_extensions/branches/main/documents/Carver_Skis_Specification_Guide.pdf;          --CHANGE THIS\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs#.public.mdt2_snowflake_extensions/branches/main/documents/Mondracer_Infant_Bike.pdf;                    --CHANGE THIS\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs#.public.mdt2_snowflake_extensions/branches/main/documents/OutPiste_Skis_Specification_Guide.pdf;        --CHANGE THIS\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs#.public.mdt2_snowflake_extensions/branches/main/documents/Premium_Bicycle_User_Guide.pdf;               --CHANGE THIS\nalter stage docs refresh;\nls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d27d4ef-f78f-4099-8c36-bcd9e35b64bf",
   "metadata": {
    "language": "sql",
    "name": "check_stream",
    "collapsed": false,
    "resultHeight": 217
   },
   "outputs": [],
   "source": "--Let's verify that the Stream sees the new documents that have landed on the stage\nselect * from cortex_search_stream;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf12e80e-c00d-4208-a915-7e5d4931576e",
   "metadata": {
    "language": "sql",
    "name": "CHUNKS_PARSE_DOC",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "/*********************************************\n Create table to hold the extracted document data\n*********************************************/\nCREATE TABLE if not exists CHUNKS_PARSE_DOC (\n     relative_path VARCHAR\n   , size NUMBER\n   , file_url VARCHAR\n   , scoped_file_url VARCHAR\n   , page_content VARIANT\n   , category VARCHAR\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b007e45-2f1a-4b44-9d0a-63a09f323d2f",
   "metadata": {
    "language": "sql",
    "name": "enable_change_tracking",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "/*********************************************\n In order to operationalize the flow, we will be creating a Dynamic Table downstream.  Dynamic Tables require change tracking to be enabled on source tables.  \n*********************************************/\nalter table chunks_parse_doc set change_tracking = True;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78fa429a-cab4-46cb-a233-30f08cd56114",
   "metadata": {
    "language": "sql",
    "name": "parse_doc_and_split_text",
    "collapsed": false,
    "resultHeight": 217
   },
   "outputs": [],
   "source": "/***************************************\n                PARSE_DOC + SPLIT_TEXT   \n\nThe SPLIT_TEXT_RECURSIVE_CHARACTER function splits a string up into smaller chunks of text recursively for \npreprocessing text as input to ML workloads. The function returns an array of text chunks, where the chunks are computed based on the input parameters provided.\n\nReturns the extracted content from a document on a Snowflake stage as an OBJECT that contains JSON-encoded objects as strings. This function supports 2 types of extractions, Optical Character Recognition (OCR) and layout. \n\nIf LAYOUT mode is selected, the data is markdown with structural content including tables.\nIf OCR, the data is text content\n***************************************/\nselect\n    relative_path\n    , size\n    , file_url\n    , GET_PRESIGNED_URL(@docs, relative_path) as scoped_file_url   \n    , SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n        to_variant(snowflake.cortex.parse_document(\n                   @docs,\n                   relative_path,\n                   {'mode': 'layout'}\n        )):content, 'MARKDOWN', 1800, 250) as page_content\n  from cortex_search_stream\n where METADATA$ACTION = 'INSERT';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1393c43-4296-44b4-904a-c84b1ae7ceb6",
   "metadata": {
    "language": "sql",
    "name": "Classify_Category",
    "collapsed": false,
    "resultHeight": 217
   },
   "outputs": [],
   "source": "/***************************************\nWe are going to use the power of Large Language Models to easily classify the documents we are ingesting in our RAG application. We are just going to use the file name but you could also use some of the content of the doc itself. Depending on your use case you may want to use different approaches. \n\nWe will pass the file name to the LLM using the Cortex Complete function with a prompt to classify what the guide refers to.  The prompt will be as simple as this but you can try to customize it depending on your use case and documents. Classification is not mandatory for Cortex Search but we want to use it here to also demo hybrid search.\n***************************************/\nselect\n    relative_path\n    , TRIM(snowflake.cortex.COMPLETE (\n            'llama3-70b',\n            'Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' ||           \n            relative_path || '</file>'), '\\n') AS category\n    from cortex_search_stream\n    where METADATA$ACTION = 'INSERT';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8872ada6-fe9c-411b-8508-98b93ce97ac2",
   "metadata": {
    "language": "sql",
    "name": "create_task",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "/***************************************\nLet's operationalize the previous two statements by putting them in a task that listens for documents to land on the stage.  When a new document lands, we will extract and chunk the contents as well as assign a category.  The extracted text is stored in a variant column.\n\nChange the warehouse name to your warehouse\nChange the database to your database\n\nChange the warehouse\n***************************************/\ncreate or replace task cortex_search_new_docs\nwarehouse = mdt2_compute_wh#                                                                          --CHANGE THIS\nschedule = '1 minute'\ncomment = 'Process new files in the stage and insert data into the chunks_parse_doc table'\nwhen SYSTEM$STREAM_HAS_DATA('cortex_search_stream') as\ninsert into CHUNKS_PARSE_DOC (\n    select\n        relative_path\n        , size\n        , file_url\n        , GET_PRESIGNED_URL(@docs, relative_path) as scoped_file_url \n        , SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n            to_variant(snowflake.cortex.parse_document(\n                       @docs,  \n                       relative_path,\n                       {'mode': 'layout'}\n            )):content, 'MARKDOWN', 1800, 250) as page_content\n        , TRIM(snowflake.cortex.COMPLETE (\n            'llama3-70b',\n            'Given the name of the file between <file> and </file> determine if it is related to bikes or snow. Use only one word <file> ' || \n             relative_path || '</file>'), '\\n') AS category\n    from cortex_search_stream\n    where METADATA$ACTION = 'INSERT'\n    );\n    \nalter task cortex_search_new_docs resume;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2da314fb-9db8-41e2-b9fd-34fb702ac86e",
   "metadata": {
    "language": "sql",
    "name": "verify_extracted_docs",
    "collapsed": false,
    "resultHeight": 217
   },
   "outputs": [],
   "source": "/**********************\nVerify that the documents have been successfully extracted\n**********************/\nselect * from chunks_parse_doc;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cea4cf1c-89b3-493d-8539-7874a9bcfe48",
   "metadata": {
    "language": "sql",
    "name": "flatten_chunks",
    "collapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "/******************************\nThe next step is the parse and flatten the extracted text.\n******************************/\nselect RELATIVE_PATH, scoped_file_url, size, file_url,\n       c.value AS chunk,\n       category\n  from CHUNKS_PARSE_DOC D, LATERAL FLATTEN(INPUT => D.page_content) c;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aeb8d557-d7ed-465d-9356-a19eb7404d55",
   "metadata": {
    "language": "sql",
    "name": "create_dynamic_table",
    "collapsed": false,
    "resultHeight": 439
   },
   "outputs": [],
   "source": "/******************************\nWe can operationalize it by using a dynamic table when new documents are loaded into the CHUNKS_PARSE_DOC table.\n\nNote that there are 18 rows on the table.\n\nChange the name of the warehouse\n******************************/\ncreate or replace dynamic table cortex_search_chunks\ntarget_lag = '1 minute'\nwarehouse = MDT2_COMPUTE_WH# as                                                                               --CHANGE THIS\n  select RELATIVE_PATH, scoped_file_url, size, file_url,\n         c.value AS chunk,\n         category\n    from CHUNKS_PARSE_DOC D, LATERAL FLATTEN(INPUT => D.page_content) c;   -- CHANGE THIS\n\n\nselect * from CORTEX_SEARCH_CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e136c1f2-fce1-4731-b27c-4efda119138e",
   "metadata": {
    "language": "sql",
    "name": "create_search_svc",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "/******************************************\nCREATE THE SEARCH SERVICE\n\nAll of the operational complexity of building the search service is abstracted into a single SQL statement for service creation. This removes the burden of creating and managing multiple processes for ingestion, embedding and serving, ultimately freeing up time to focus on developing cutting-edge AI applications.\n******************************************/\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE cortex_search_svc ON chunk\nATTRIBUTES CATEGORY\nWAREHOUSE = compute_wh                                                      --CHANGE THIS\nTARGET_LAG = '1 minute'\nAS (\nSELECT\n    chunk::varchar as chunk,\n    relative_path,\n    file_url,\n    category\nFROM\n    CORTEX_SEARCH_CHUNKS\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b6a9880-0801-4157-b491-f4eafb7fe791",
   "metadata": {
    "language": "sql",
    "name": "Last_4_docs",
    "collapsed": false,
    "resultHeight": 357
   },
   "outputs": [],
   "source": "/******************************\nLet's test the pipeline.  We will add the last 4 documents and verify that they are processed in an automated fashion\n\nChange the name of the database\n******************************/\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs.public.mdt2_snowflake_extensions/branches/main/documents/RacingFast_Skis_Specification_Guide.pdf;           --CHANGE THIS\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs.public.mdt2_snowflake_extensions/branches/main/documents/Ski_Boots_TDBootz_Special.pdf;                     --CHANGE THIS\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs.public.mdt2_snowflake_extensions/branches/main/documents/The_Ultimate_Downhill_Bike.pdf;                    --CHANGE THIS\nCOPY FILES\n  INTO @docs\n  FROM @mdt2_cortex_search_docs.public.mdt2_snowflake_extensions/branches/main/documents/The_Xtreme_Road_Bike_105_SL.pdf;                   --CHANGE THIS\nalter stage docs refresh;\nls @docs;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b81ac52-1ebc-4d36-b368-a538f32832a0",
   "metadata": {
    "language": "sql",
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "/******************************\nCheck the CORTEX_SEARCH_CHUNKS table after a couple of minutes to ensure that the new documents have been processed.\n******************************/\nselect * from CORTEX_SEARCH_CHUNKS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "82282d1f-13bd-45cd-b897-b388ad4492c7",
   "metadata": {
    "name": "streamlit_app",
    "collapsed": false,
    "resultHeight": 4188
   },
   "source": "# Next Step:\n\nNow that the Cortex Search service has been created, we need to create an app to access the API in order to allow users to chat with the documents.  We will do this using Streamlit.  \n\n1. Copy the code below\n2. Create a new Streamlit app\n3. Edit the app and replace the Streamlit code with the code below\n4. Change the DATABASE to your database\n5. Add the snowflake.core package\n6. Click Run\n\n```\nimport streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\n\nfrom snowflake.core import Root\n\nimport pandas as pd\nimport json\n\npd.set_option(\"max_colwidth\",None)\n\n### Default Values\nNUM_CHUNKS = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\n\n# service parameters\nCORTEX_SEARCH_DATABASE = \"MDT2_CORTEX_SEARCH_DOCS##\"\nCORTEX_SEARCH_SCHEMA = \"DATA\"\nCORTEX_SEARCH_SERVICE = \"CORTEX_SEARCH_SVC\"\n######\n######\n\n# columns to query in the service\nCOLUMNS = [\n    \"chunk\",\n    \"relative_path\",\n    \"category\"\n]\n\nsession = get_active_session()\nroot = Root(session)                         \n\nsvc = root.databases[CORTEX_SEARCH_DATABASE].schemas[CORTEX_SEARCH_SCHEMA].cortex_search_services[CORTEX_SEARCH_SERVICE]\n   \n### Functions\n     \ndef config_options():\n\n    st.sidebar.selectbox('Select your model:',(\n                                    'mixtral-8x7b',\n                                    'snowflake-arctic',\n                                    'mistral-large',\n                                    'llama3-8b',\n                                    'llama3-70b',\n                                    'reka-flash',\n                                     'mistral-7b',\n                                     'llama2-70b-chat',\n                                     'gemma-7b'), key=\"model_name\")\n\n    categories = session.sql(\"select category from cortex_search_chunks group by category\").collect()\n\n    cat_list = ['ALL']\n    for cat in categories:\n        cat_list.append(cat.CATEGORY)\n            \n    st.sidebar.selectbox('Select what products you are looking for', cat_list, key = \"category_value\")\n\n    st.sidebar.expander(\"Session State\").write(st.session_state)\n\ndef get_similar_chunks_search_service(query):\n\n    if st.session_state.category_value == \"ALL\":\n        response = svc.search(query, COLUMNS, limit=NUM_CHUNKS)\n    else: \n        filter_obj = {\"@eq\": {\"category\": st.session_state.category_value} }\n        response = svc.search(query, COLUMNS, filter=filter_obj, limit=NUM_CHUNKS)\n\n    st.sidebar.json(response.json())\n    \n    return response.json()  \n\ndef create_prompt (myquestion):\n\n    if st.session_state.rag == 1:\n        prompt_context = get_similar_chunks_search_service(myquestion)\n  \n        prompt = f\"\"\"\n           You are an expert chat assistance that extracs information from the CONTEXT provided\n           between <context> and </context> tags.\n           When ansering the question contained between <question> and </question> tags\n           be concise and do not hallucinate. \n           If you don´t have the information just say so.\n           Only anwer the question if you can extract it from the CONTEXT provideed.\n           \n           Do not mention the CONTEXT used in your answer.\n    \n           <context>          \n           {prompt_context}\n           </context>\n           <question>  \n           {myquestion}\n           </question>\n           Answer: \n           \"\"\"\n\n        json_data = json.loads(prompt_context)\n\n        relative_paths = set(item['relative_path'] for item in json_data['results'])\n        \n    else:     \n        prompt = f\"\"\"[0]\n         'Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n        relative_paths = \"None\"\n            \n    return prompt, relative_paths\n\ndef complete(myquestion):\n\n    prompt, relative_paths =create_prompt (myquestion)\n    cmd = \"\"\"\n            select snowflake.cortex.complete(?, ?) as response\n          \"\"\"\n    \n    df_response = session.sql(cmd, params=[st.session_state.model_name, prompt]).collect()\n    return df_response, relative_paths\n\ndef main():\n    \n    st.title(f\":speech_balloon: Chat Document Assistant with Snowflake Cortex\")\n    st.write(\"This is the list of documents you already have and that will be used to answer your questions:\")\n    docs_available = session.sql(\"ls @docs\").collect()\n    list_docs = []\n    for doc in docs_available:\n        list_docs.append(doc[\"name\"])\n    st.dataframe(list_docs)\n\n    config_options()\n\n    st.session_state.rag = st.sidebar.checkbox('Use your own documents as context?')\n\n    question = st.text_input(\"Enter question\", placeholder=\"Is there any special lubricant to be used with the premium bike?\", label_visibility=\"collapsed\")\n\n    if question:\n        response, relative_paths = complete(question)\n        res_text = response[0].RESPONSE\n        st.markdown(res_text)\n\n        if relative_paths != \"None\":\n            with st.sidebar.expander(\"Related Documents\"):\n                for path in relative_paths:\n                    cmd2 = f\"select GET_PRESIGNED_URL(@docs, '{path}', 360) as URL_LINK from directory(@docs)\"\n                    df_url_link = session.sql(cmd2).to_pandas()\n                    url_link = df_url_link._get_value(0,'URL_LINK')\n        \n                    display_url = f\"Doc: [{path}]({url_link})\"\n                    st.sidebar.markdown(display_url)\n                \nif __name__ == \"__main__\":\n    main()\n```"
  },
  {
   "cell_type": "markdown",
   "id": "187beb55-b8c1-4bfa-8aa2-88864edb5861",
   "metadata": {
    "name": "multiturn_streamlit_app",
    "collapsed": false,
    "resultHeight": 4188
   },
   "source": "# One more thing:\n\nThe first app that we created searched documents one question at a time with no context as to the converation history.  We can also pass the chat history to the LLM to create more a converational approach.\n\n1. Copy the code below\n2. Create a new Streamlit app\n3. Edit the app and replace the Streamlit code with the code below\n4. Change the DATABASE to your database\n5. Add the snowflake.core package\n6. Click Run\n\n```\nimport streamlit as st # Import python packages\nfrom snowflake.snowpark.context import get_active_session\n\nfrom snowflake.core import Root\n\nimport pandas as pd\nimport json\n\npd.set_option(\"max_colwidth\",None)\n\n### Default Values\nNUM_CHUNKS = 3 # Num-chunks provided as context. Play with this to check how it affects your accuracy\n\n# service parameters\nCORTEX_SEARCH_DATABASE = \"MDT2_CORTEX_SEARCH_DOCS##\"\nCORTEX_SEARCH_SCHEMA = \"DATA\"\nCORTEX_SEARCH_SERVICE = \"CORTEX_SEARCH_SVC\"\n######\n######\n\n# columns to query in the service\nCOLUMNS = [\n    \"chunk\",\n    \"relative_path\",\n    \"category\"\n]\n\nsession = get_active_session()\nroot = Root(session)                         \n\nsvc = root.databases[CORTEX_SEARCH_DATABASE].schemas[CORTEX_SEARCH_SCHEMA].cortex_search_services[CORTEX_SEARCH_SERVICE]\n   \n### Functions\n     \ndef config_options():\n\n    st.sidebar.selectbox('Select your model:',(\n                                    'mixtral-8x7b',\n                                    'snowflake-arctic',\n                                    'mistral-large',\n                                    'llama3-8b',\n                                    'llama3-70b',\n                                    'reka-flash',\n                                     'mistral-7b',\n                                     'llama2-70b-chat',\n                                     'gemma-7b'), key=\"model_name\")\n\n    categories = session.sql(\"select category from cortex_search_chunks group by category\").collect()\n\n    cat_list = ['ALL']\n    for cat in categories:\n        cat_list.append(cat.CATEGORY)\n            \n    st.sidebar.selectbox('Select what products you are looking for', cat_list, key = \"category_value\")\n\n    st.sidebar.expander(\"Session State\").write(st.session_state)\n\ndef get_similar_chunks_search_service(query):\n\n    if st.session_state.category_value == \"ALL\":\n        response = svc.search(query, COLUMNS, limit=NUM_CHUNKS)\n    else: \n        filter_obj = {\"@eq\": {\"category\": st.session_state.category_value} }\n        response = svc.search(query, COLUMNS, filter=filter_obj, limit=NUM_CHUNKS)\n\n    st.sidebar.json(response.json())\n    \n    return response.json()  \n\ndef create_prompt (myquestion):\n\n    if st.session_state.rag == 1:\n        prompt_context = get_similar_chunks_search_service(myquestion)\n  \n        prompt = f\"\"\"\n           You are an expert chat assistance that extracs information from the CONTEXT provided\n           between <context> and </context> tags.\n           When ansering the question contained between <question> and </question> tags\n           be concise and do not hallucinate. \n           If you don´t have the information just say so.\n           Only anwer the question if you can extract it from the CONTEXT provideed.\n           \n           Do not mention the CONTEXT used in your answer.\n    \n           <context>          \n           {prompt_context}\n           </context>\n           <question>  \n           {myquestion}\n           </question>\n           Answer: \n           \"\"\"\n\n        json_data = json.loads(prompt_context)\n\n        relative_paths = set(item['relative_path'] for item in json_data['results'])\n        \n    else:     \n        prompt = f\"\"\"[0]\n         'Question:  \n           {myquestion} \n           Answer: '\n           \"\"\"\n        relative_paths = \"None\"\n            \n    return prompt, relative_paths\n\ndef complete(myquestion):\n\n    prompt, relative_paths =create_prompt (myquestion)\n    cmd = \"\"\"\n            select snowflake.cortex.complete(?, ?) as response\n          \"\"\"\n    \n    df_response = session.sql(cmd, params=[st.session_state.model_name, prompt]).collect()\n    return df_response, relative_paths\n\ndef main():\n    \n    st.title(f\":speech_balloon: Chat Document Assistant with Snowflake Cortex\")\n    st.write(\"This is the list of documents you already have and that will be used to answer your questions:\")\n    docs_available = session.sql(\"ls @docs\").collect()\n    list_docs = []\n    for doc in docs_available:\n        list_docs.append(doc[\"name\"])\n    st.dataframe(list_docs)\n\n    config_options()\n\n    st.session_state.rag = st.sidebar.checkbox('Use your own documents as context?')\n\n    question = st.text_input(\"Enter question\", placeholder=\"Is there any special lubricant to be used with the premium bike?\", label_visibility=\"collapsed\")\n\n    if question:\n        response, relative_paths = complete(question)\n        res_text = response[0].RESPONSE\n        st.markdown(res_text)\n\n        if relative_paths != \"None\":\n            with st.sidebar.expander(\"Related Documents\"):\n                for path in relative_paths:\n                    cmd2 = f\"select GET_PRESIGNED_URL(@docs, '{path}', 360) as URL_LINK from directory(@docs)\"\n                    df_url_link = session.sql(cmd2).to_pandas()\n                    url_link = df_url_link._get_value(0,'URL_LINK')\n        \n                    display_url = f\"Doc: [{path}]({url_link})\"\n                    st.sidebar.markdown(display_url)\n                \nif __name__ == \"__main__\":\n    main()\n```"
  }
 ]
}